{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6378dcd6-9bb7-48d1-a095-7727a24b7173",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "- Follow the installation instructions in the readme file\n",
    "- Answer the questions in this notebook\n",
    "- Once your work is finished: restart the kernel, run all cells in order and check that the outputs are correct.\n",
    "- Send your completed notebook to `remy.degenne@inria.fr` with email title `SL_TP4_NAME1_NAME2` (or `SL_TP4_NAME` if you work alone).\n",
    "\n",
    "**Deadline: January 23, 15:00 CET**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e22d328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is setting up google colab. Ignore it if you work locally.\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    print(\"Installing packages, please wait a few moments. Restart the runtime after the installation.\")\n",
    "    # install rlberry library\n",
    "    !pip install scipy scikit_learn git+https://github.com/rlberry-py/rlberry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef0b14a-6e93-4f0b-be94-84d090c5d184",
   "metadata": {},
   "source": [
    "# Fitted Q Iteration (FQI)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, you will implement the Fitted Q Iteration algorithm (FQI) to solve the [CartPole](https://gymnasium.farama.org/environments/classic_control/cart_pole/) problem.\n",
    "\n",
    "This notebooks will first cover the basics for using the Gymnasium library: how to instantiate an environment, step into it and collect training data from the FQI algorithm.\n",
    "\n",
    "You will then learn how to implement step-by-step the FQI algorithm which is the predecessor of the [Deep Q-Network (DQN)](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "008852aa-cf8f-4c81-afaf-7d3be75f389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import os\n",
    "\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.base import RegressorMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2369b55-266c-4dbe-b14d-250ef7386407",
   "metadata": {},
   "source": [
    "## First steps with the Gym interface\n",
    "\n",
    "An environment that follows the [gym interface](https://gymnasium.farama.org/) is quite simple to use.\n",
    "It provides to this user mainly three methods, which have the following signature (for gym versions > 0.26):\n",
    "\n",
    "- `reset()` called at the beginning of an episode, it returns an observation and a dictionary with additional info (defaults to an empty dict)\n",
    "- `step(action)` called to take an action with the environment, it returns the next observation, the immediate reward, whether new state is a terminal state (episode is finished), whether the max number of timesteps is reached (episode is artificially finished), and additional information\n",
    "- (Optional) `render()` which allow to visualize the agent in action. Note that graphical interface does not work on google colab, so we cannot use it directly (we have to rely on `render_mode='rbg_array'` to retrieve an image of the scene).\n",
    "\n",
    "Under the hood, it also contains two useful properties:\n",
    "- `observation_space` which is one of the gym spaces (`Discrete`, `Box`, ...) and describe the type and shape of the observation\n",
    "- `action_space` which is also a gym space object that describes the action space, so the type of action that can be taken\n",
    "\n",
    "The best way to learn about [gym spaces](https://gymnasium.farama.org/api/spaces/) is to look at the [source code](https://github.com/Farama-Foundation/Gymnasium/tree/main/gymnasium/spaces), but you need to know at least the main ones:\n",
    "- `gym.spaces.Box`: A (possibly unbounded) box in $R^n$. Specifically, a Box represents the Cartesian product of n closed intervals. Each interval has the form of one of $[a, b]$, $(-\\infty, b]$, $[a, \\infty)$, or $(-\\infty, \\infty)$. Example: A 1D-Vector or an image observation can be described with the Box space.\n",
    "```python\n",
    "# Example for using image as input:\n",
    "observation_space = spaces.Box(low=0, high=255, shape=(HEIGHT, WIDTH, N_CHANNELS), dtype=np.uint8)\n",
    "```                                       \n",
    "\n",
    "- `gym.spaces.Discrete`: A discrete space in $\\{ 0, 1, \\dots, n-1 \\}$\n",
    "  Example: if you have two actions (\"left\" and \"right\") you can represent your action space using `Discrete(2)`, the first action will be 0 and the second 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af049724-3db9-4dec-a40c-3aa025735f00",
   "metadata": {},
   "source": [
    "## CartPole Environment\n",
    "\n",
    "For this example, we will use CartPole environment, a classic control problem.\n",
    "\n",
    "\"A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. \"\n",
    "\n",
    "Cartpole environment: [https://gymnasium.farama.org/environments/classic_control/cart_pole/](https://gymnasium.farama.org/environments/classic_control/cart_pole/)\n",
    "\n",
    "![Cartpole](https://cdn-images-1.medium.com/max/1143/1*h4WTQNVIsvMXJTCpXm_TAw.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60ec422f-2edd-40df-8da2-c1093e1da38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the environment\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "476f88b6-4fe2-45ec-bddd-4d83feb3a86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "Shape: (4,)\n",
      "Action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "# Box(4,) means that it is a Vector with 4 components\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Shape:\", env.observation_space.shape)\n",
    "\n",
    "# Discrete(2) means that there is two discrete actions\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e414990e-2cbd-4f6f-b268-42f16a9b253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reset method is called at the beginning of an episode\n",
    "obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e89a95c8-d64a-43a3-adee-344891bbd1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled action: 0\n"
     ]
    }
   ],
   "source": [
    "# Sample a random action\n",
    "action = env.action_space.sample()\n",
    "print(f\"Sampled action: {action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "311777a2-9518-496a-b1e8-0eb1af81fb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step in the environment\n",
    "obs, reward, terminated, truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5102a9c8-8407-4f88-b58d-4e3e5a00af3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,) 1.0 False False {}\n"
     ]
    }
   ],
   "source": [
    "# Note the obs is a numpy array\n",
    "# info is an empty dict for now but can contain any debugging info\n",
    "# reward is a scalar\n",
    "print(obs.shape, reward, terminated, truncated, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fbec50-1da0-4bc4-8b88-79fa38480bfe",
   "metadata": {},
   "source": [
    "### Exercise: write the function to collect data\n",
    "\n",
    "This function collects a dataset of transitions that will be used to train a model using the FQI algorithm.\n",
    "\n",
    "See docstring of the function for what is expected as input/output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f1b3ff1-911b-4582-91f7-49420380eda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class OfflineData:\n",
    "    \"\"\"\n",
    "    A class to store transitions.\n",
    "    \"\"\"\n",
    "\n",
    "    observations: np.ndarray  # same as \"state\" in the theory\n",
    "    next_observations: np.ndarray\n",
    "    actions: np.ndarray\n",
    "    rewards: np.ndarray\n",
    "    terminateds: np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9f753ed-5f5d-4133-ab82-c84b3cfe2eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(env: gym.Env, n_steps: int = 50_000) -> OfflineData:\n",
    "    \"\"\"\n",
    "    Collect transitions using a random agent (sample action randomly).\n",
    "\n",
    "    :param env: The environment.\n",
    "    :param n_steps: Number of steps to perform in the env.\n",
    "    :return: The collected transitions.\n",
    "    \"\"\"\n",
    "\n",
    "    assert isinstance(env.observation_space, spaces.Box)\n",
    "    # Numpy arrays (buffers) to collect the data\n",
    "    observations = np.zeros((n_steps, *env.observation_space.shape))\n",
    "    next_observations = np.zeros((n_steps, *env.observation_space.shape))\n",
    "    # Discrete actions\n",
    "    actions = np.zeros((n_steps, 1))\n",
    "    rewards = np.zeros((n_steps,))\n",
    "    terminateds = np.zeros((n_steps,))\n",
    "\n",
    "    # Variable to know if the episode is over (done = terminated or truncated)\n",
    "    done = False\n",
    "    # Start the first episode\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    # You need to collect transitions for `n_steps` using\n",
    "    # a random agent (sample action uniformly).\n",
    "    # Do not forget to reset the environment if the current episode is over\n",
    "    # (done = terminated or truncated)\n",
    "    #\n",
    "    # TODO:\n",
    "    # 1. Sample a random action\n",
    "    # 2. Step in the env using this random action\n",
    "    # 3. Retrieve the new transition data (observation, reward, ...)\n",
    "    #  and update the numpy arrays (buffers)\n",
    "    # 4. Repeat until you collected `n_steps` transitions\n",
    "\n",
    "    for i in range(n_steps):\n",
    "        action = env.action_space.sample()\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        observations[i] = obs\n",
    "        next_observations[i] = next_obs\n",
    "        actions[i] = action\n",
    "        rewards[i] = reward\n",
    "        terminateds[i] = terminated or truncated\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "        if terminated or truncated:\n",
    "            obs, _ = env.reset()\n",
    "\n",
    "    \n",
    "\n",
    "    ### END OF YOUR CODE\n",
    "\n",
    "    return OfflineData(\n",
    "        observations,\n",
    "        next_observations,\n",
    "        actions,\n",
    "        rewards,\n",
    "        terminateds,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe5d4b3-d3d6-4f82-9337-a8566c1fc707",
   "metadata": {},
   "source": [
    "Let's try the collect data method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f499a940-a35f-4b8c-86bb-94231c41a87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"CartPole-v1\"\n",
    "env = gym.make(env_id)\n",
    "n_steps = 10_000\n",
    "# Collect transitions for n_steps\n",
    "data = collect_data(env=env, n_steps=n_steps)\n",
    "\n",
    "# Check the length of the collected data\n",
    "assert len(data.observations) == n_steps\n",
    "assert len(data.actions) == n_steps\n",
    "# Check that there are multiple episodes in the data\n",
    "assert not np.all(data.terminateds)\n",
    "assert np.any(data.terminateds)\n",
    "# Check the shape of the collected data\n",
    "if env_id == \"CartPole-v1\":\n",
    "    assert data.observations.shape == (n_steps, 4)\n",
    "    assert data.next_observations.shape == (n_steps, 4)\n",
    "assert data.actions.shape == (n_steps, 1)\n",
    "assert data.rewards.shape == (n_steps,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239ad0de-bec9-4918-a691-4322d062c45a",
   "metadata": {},
   "source": [
    "## Fitted Q Iteration (FQI) Agent\n",
    "\n",
    "See Lecture 4, slide 31 (and next slides for more explanations in the linear case, although this practical session is not linear).\n",
    "\n",
    "At each iteration of the algorithm, a dataset of transitions is gathered. Then target Q values for each transition are computed and the algorithm solves a regression problem with the transitions as inputs and the target values as outputs to update its Q-value approximation.\n",
    "\n",
    "After the maximal number of iterations is reached, the policy returned is the greedy policy with respect to the current Q-values.\n",
    "\n",
    "### Choosing a regression model\n",
    "\n",
    "With FQI, you can use any regression model to produce a Q-value estimator from a dataset of transitions and targets.\n",
    "\n",
    "Here we are choosing a [k-nearest neighbors regressor](https://scikit-learn.org/stable/modules/neighbors.html#regression), but one could choose a linear model, a decision tree, a neural network, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d8d873e-bc40-4a35-b98b-4fe9ce916aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First choose the regressor\n",
    "model_class = partial(\n",
    "    KNeighborsRegressor, n_neighbors=30\n",
    ")  # LinearRegression, GradientBoostingRegressor..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf9544c-87f4-4872-9ae9-eef49c3a040a",
   "metadata": {},
   "source": [
    "### 1. Exercise: write the function to predict Q-Values\n",
    "\n",
    "In FQI, we will need to compute, for any transition $(s, a, r, s')$, a target value $y = r + \\gamma \\cdot \\max_{a' \\in A}(Q^{n-1}_\\theta(s', a'))$. In order to do that, we need to be able to compute the current Q-value of state-action pairs.\n",
    "\n",
    "See docstring of the function for what is expected as input/output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38316a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_values(\n",
    "    model: RegressorMixin,\n",
    "    obs: np.ndarray,\n",
    "    n_actions: int,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Retrieve the q-values for a set of observations obs.\n",
    "    Q(s, action) for all s in obs and all possible actions.\n",
    "\n",
    "    :param model: Q-value estimator\n",
    "    :param obs: A batch of observations\n",
    "    :param n_actions: Number of discrete actions.\n",
    "    :return: The predicted q-values for the given observations\n",
    "        (batch_size, n_actions)\n",
    "    \"\"\"\n",
    "    batch_size = len(obs)\n",
    "    q_values = np.zeros((batch_size, n_actions))\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    # TODO: for every possible actions a:\n",
    "    # 1. Create the regression model input $(s, a)$ for the action a\n",
    "    # and states s (here a batch of observations)\n",
    "    # 2. Predict the q-values for the batch of states (use model.predict)\n",
    "    # 3. Update q-values array for the current action a\n",
    "\n",
    "    # For each action, predict the Q-value for all observations\n",
    "    for action in range(n_actions):\n",
    "        # Create model input (s, a) for each observation and current action\n",
    "        model_input = np.hstack((obs, np.full((batch_size, 1), action)))\n",
    "\n",
    "        # Predict Q-values for this action across all observations\n",
    "        q_values[:, action] = model.predict(model_input)\n",
    "\n",
    "    ### END OF YOUR CODE\n",
    "\n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7a24b4-a416-4607-af3a-9dfdbc203fc1",
   "metadata": {},
   "source": [
    "### Create the Agent\n",
    "To create an agent, rlberry requires to use a **very simple interface**, with basically two methods to implement: `fit()` and `eval()`.\n",
    "\n",
    "You can find more information on this interface [here(AgentWithSimplePolicy)](rlberry.agents.agent.AgentWithSimplePolicy).\n",
    "### function fit() :\n",
    "\n",
    "#### 1. First Iteration\n",
    "\n",
    "For $n = 0$, the initial training set is defined as:\n",
    "\n",
    "- $x = (s_t, a_t)$\n",
    "- $y = r_t$\n",
    "\n",
    "We fit a regression model $f_\\theta(x) = y$ to obtain $ Q^{n=0}_\\theta(s, a) $\n",
    "\n",
    "\n",
    "\n",
    "#### 2. Exercise: the fitted Q iterations\n",
    "\n",
    "1. Create the training set based on the previous iteration $ Q^{n-1}_\\theta(s, a) $ and the transitions:\n",
    "- input: $x = (s_t, a_t)$\n",
    "- if $s_{t+1}$ is non-terminal: $y = r_t + \\gamma \\cdot \\max_{a' \\in A}(Q^{n-1}_\\theta(s_{t+1}, a'))$\n",
    "- if $s_{t+1}$ is terminal, do not bootstrap: $y = r_t$\n",
    "\n",
    "2. Fit a model $f_\\theta$ using a regression algorithm to obtain $ Q^{n}_\\theta(s, a)$\n",
    " \n",
    "\\begin{aligned}\n",
    " f_\\theta(x) = y\n",
    "\\end{aligned}\n",
    "\n",
    "4. Repeat, $n = n + 1$\n",
    "\n",
    "### function evaluate() :\n",
    "\n",
    "#### 3. Exercise: write the function to evaluate a model\n",
    "\n",
    "A greedy policy $\\pi(s)$ can be defined using the q-value:\n",
    "\n",
    "$\\pi(s) = argmax_{a \\in A} Q(s, a)$.\n",
    "\n",
    "It is the policy that take the action with the highest q-value for a given state.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fe29bd2-ea95-4778-b466-537a088615b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlberry.agents import Agent\n",
    "from gymnasium.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "\n",
    "class Fitted_Q_Iteration(Agent):\n",
    "    name = \"Fitted_Q_Iteration\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        model_class,\n",
    "        n_steps_collection=50_000,  # Number of steps to perform in the env to collect data.\n",
    "        eval_freq=2,\n",
    "        n_eval_episodes=10,\n",
    "        gamma=0.99,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # it's important to put **kwargs to ensure compatibility with the base class\n",
    "        # self.env is initialized here\n",
    "        super().__init__(env=env, **kwargs)\n",
    "\n",
    "        self.model_class = model_class  # The Agent's regression model class\n",
    "        self.eval_freq = eval_freq  # How often do we evaluate the learned model\n",
    "        self.n_eval_episodes = n_eval_episodes  # How many episodes to evaluate every eval-freq\n",
    "        \n",
    "        self.gamma = 0.99  # discount factor\n",
    "        self.n_actions = int(self.env.action_space.n)  # Number of discrete actions\n",
    "\n",
    "        # Collect data : transitions for n_steps\n",
    "        self.current_data = collect_data(env=self.env, n_steps=n_steps_collection)\n",
    "\n",
    "        # 1 - First iteration:\n",
    "        # The target q-value is the reward obtained\n",
    "        targets = self.current_data.rewards.copy()\n",
    "        # Create input for current observations and actions\n",
    "        # Concatenate the observations and actions\n",
    "        # so we can predict qf(s_t, a_t)\n",
    "        self.current_obs_input = np.concatenate(\n",
    "            (self.current_data.observations, self.current_data.actions), axis=1\n",
    "        )\n",
    "        # Fit the estimator for the current target\n",
    "        self.current_model = model_class().fit(self.current_obs_input, targets)\n",
    "\n",
    "    def fit(self, budget, **kwargs):\n",
    "        # 2 - the fitted Q iterations\n",
    "        for iter_idx in range(budget):\n",
    "            ### YOUR CODE HERE\n",
    "            # TODO:\n",
    "            # 1. Compute the q values for the next states using\n",
    "            # the previous regression model\n",
    "            # 2. Keep only the next q values that correspond\n",
    "            # to the greedy-policy\n",
    "            # 3. Construct the regression target (TD(0) target)\n",
    "            # 4. Fit a new regression model with this new target\n",
    "\n",
    "            next_q_values = get_q_values(self.current_model, self.current_data.next_observations, self.n_actions)\n",
    "            \n",
    "            # Select max Q-value for each next observation\n",
    "            max_next_q_values = np.max(next_q_values, axis=1)\n",
    "            \n",
    "            # Construct TD(0) target\n",
    "            targets = self.current_data.rewards + self.gamma * max_next_q_values * (1 - self.current_data.terminateds)\n",
    "            \n",
    "            # Fit new regression model\n",
    "            self.current_model = self.model_class().fit(self.current_obs_input, targets)\n",
    "            \n",
    "\n",
    "            ### END OF YOUR CODE\n",
    "\n",
    "            if (iter_idx + 1) % self.eval_freq == 0:\n",
    "                print(f\"Iter {iter_idx + 1}\")\n",
    "                print(\n",
    "                    f\"Score: {self.current_model.score(self.current_obs_input, targets):.2f}\"\n",
    "                )\n",
    "                final_eval_result = self.eval(self.n_eval_episodes)\n",
    "\n",
    "        info = {\"final_eval_result\": final_eval_result}\n",
    "        return info  # return the final eval mean, but more informations are directly displayed by the eval() function in the output terminal.\n",
    "    \n",
    "    # 3 - function to evaluate a model\n",
    "    def eval(\n",
    "        self,\n",
    "        n_simulations: int = 10,\n",
    "        video_name: Optional[str] = None,\n",
    "    ) -> None:\n",
    "        episode_returns, episode_reward = [], 0.0\n",
    "        total_episodes = 0\n",
    "        done = False\n",
    "\n",
    "        if not self.eval_env:\n",
    "            self.eval_env = self.env\n",
    "\n",
    "        # Setup video recorder\n",
    "        video_recorder = None\n",
    "        if video_name is not None and self.eval_env.render_mode == \"rgb_array\":\n",
    "            os.makedirs(\"./logs/videos/\", exist_ok=True)\n",
    "\n",
    "            video_recorder = VideoRecorder(\n",
    "                env=self.eval_env,\n",
    "                base_path=f\"./logs/videos/{video_name}\",\n",
    "            )\n",
    "\n",
    "        obs, _ = self.eval_env.reset()\n",
    "        n_actions = int(self.eval_env.action_space.n)\n",
    "        assert isinstance(\n",
    "            self.eval_env.action_space, spaces.Discrete\n",
    "        ), \"FQI only support discrete actions\"\n",
    "\n",
    "        while total_episodes < n_simulations:\n",
    "            # Record video\n",
    "            if video_recorder is not None:\n",
    "                video_recorder.capture_frame()\n",
    "\n",
    "            ### YOUR CODE HERE\n",
    "\n",
    "            # Retrieve the q-values for the current observation\n",
    "            # you need to re-use `get_q_values()`\n",
    "            # Then select the action that maximizes the q-value for each state\n",
    "            # Do a step in the env using the selected action\n",
    "            \n",
    "            # Retrieve the q-values for the current observation\n",
    "            q_values = get_q_values(self.current_model, obs.reshape(1, -1), n_actions)\n",
    "\n",
    "            # Select the action that maximizes the q-value for the current state\n",
    "            action = np.argmax(q_values)\n",
    "\n",
    "            # Do a step in the environment using the selected action\n",
    "            #next_obs, reward, terminated, truncated, _ = self.eval_env.step(action)\n",
    "            obs, reward, terminated, truncated, _ = self.eval_env.step(action)\n",
    "            \n",
    "\n",
    "            ### END OF YOUR CODE\n",
    "\n",
    "            episode_reward += float(reward)\n",
    "\n",
    "            done = terminated or truncated\n",
    "            if done:\n",
    "                episode_returns.append(episode_reward)\n",
    "                episode_reward = 0.0\n",
    "                total_episodes += 1\n",
    "                obs, _ = self.eval_env.reset()\n",
    "            #else : \n",
    "             #   obs = next_obs  # Update the observation\n",
    "\n",
    "\n",
    "        if video_recorder is not None:\n",
    "            print(f\"Saving video to {video_recorder.path}\")\n",
    "            video_recorder.close()\n",
    "\n",
    "        print(\n",
    "            f\"Total reward = {np.mean(episode_returns):.2f} +/- {np.std(episode_returns):.2f}\"\n",
    "        )\n",
    "\n",
    "        return np.mean(episode_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d824b98-6360-4d4e-a3e2-584c0c36665e",
   "metadata": {},
   "source": [
    "### Performing experiments\n",
    "\n",
    "First, let's define some constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c16a4e16-e2a8-432f-885f-27fbe0f61f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlberry.envs import gym_make\n",
    "from rlberry.manager import ExperimentManager\n",
    "\n",
    "# Max number of iterations\n",
    "n_iterations = 20\n",
    "# How often do we evaluate the learned model\n",
    "eval_freq = 2\n",
    "# How many episodes to evaluate every eval-freq\n",
    "n_eval_episodes = 10\n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "# Number of discrete actions\n",
    "n_actions = int(env.action_space.n)\n",
    "\n",
    "\n",
    "env_id = \"CartPole-v1\"  # Id of the environment\n",
    "env_ctor = gym_make  # constructor for the env\n",
    "env_kwargs = dict(id=env_id)  # give the id of the env inside the kwargs\n",
    "\n",
    "eval_env_kwargs = dict(id=env_id, render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeaf39d-0a48-4c68-aa5b-7b734e7bcc7f",
   "metadata": {},
   "source": [
    "Now let's create an `ExperimentManager`, which is a class used to run experiments with specified agents and environments.\n",
    "\n",
    "The experiment manager spawns agents and environments for training and then once the agents are trained, it uses these agents and new environments to evaluate how well the agents perform. All of these steps can be done several times to assess stochasticity of agents and/or environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7cbff0d8-eab0-4cec-847e-23bc1c509359",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_experiment = ExperimentManager(\n",
    "    Fitted_Q_Iteration,  # Agent Class\n",
    "    (env_ctor, env_kwargs),  # Environment as Tuple(constructor,kwargs)\n",
    "    init_kwargs=dict(\n",
    "        model_class=model_class,\n",
    "        n_steps_collection=50_000,\n",
    "        eval_freq=eval_freq,\n",
    "        n_eval_episodes=n_eval_episodes,\n",
    "        gamma=gamma,\n",
    "    ),\n",
    "    eval_env=(env_ctor, eval_env_kwargs),\n",
    "    fit_budget=int(n_iterations),  # Budget used to call our agent \"fit()\"\n",
    "    n_fit=1,  # Number of agent instances to fit.\n",
    "    agent_name=\"Agent_FQI_\" + env_id,  # Name of the agent\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83f248b-14d3-468e-a218-d1164740006c",
   "metadata": {},
   "source": [
    "Use `fit()` to train an agent and then `eval_agents()` to evaluate the trained agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "594a49dc-8187-4b13-9dcc-cd49148673ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ExperimentManager fit() for Agent_FQI_CartPole-v1 with n_fit = 1 and max_workers = None."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2\n",
      "Score: 0.73\n",
      "Total reward = 22.20 +/- 9.89\n",
      "Iter 4\n",
      "Score: 0.86\n",
      "Total reward = 135.90 +/- 9.45\n",
      "Iter 6\n",
      "Score: 0.90\n",
      "Total reward = 157.80 +/- 27.95\n",
      "Iter 8\n",
      "Score: 0.92\n",
      "Total reward = 177.70 +/- 30.62\n",
      "Iter 10\n",
      "Score: 0.93\n",
      "Total reward = 173.60 +/- 27.33\n",
      "Iter 12\n",
      "Score: 0.93\n",
      "Total reward = 190.70 +/- 48.67\n",
      "Iter 14\n",
      "Score: 0.94\n",
      "Total reward = 337.20 +/- 58.65\n",
      "Iter 16\n",
      "Score: 0.94\n",
      "Total reward = 419.00 +/- 85.17\n",
      "Iter 18\n",
      "Score: 0.94\n",
      "Total reward = 361.90 +/- 100.73\n",
      "Iter 20\n",
      "Score: 0.94\n",
      "Total reward = 400.10 +/- 88.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanivbenichou/miniforge3/envs/rltp/lib/python3.9/site-packages/gymnasium/wrappers/monitoring/video_recorder.py:178: UserWarning: \u001b[33mWARN: Unable to save last video! Did you call close()?\u001b[0m\n",
      "  logger.warn(\"Unable to save last video! Did you call close()?\")\n",
      "\u001b[38;21m[INFO] 11:49: ... trained! \u001b[0m\n",
      "\u001b[38;21m[INFO] 11:49: Saved ExperimentManager(Agent_FQI_CartPole-v1) using pickle. \u001b[0m\n",
      "\u001b[38;21m[INFO] 11:49: The ExperimentManager was saved in : 'rlberry_data/temp/manager_data/Agent_FQI_CartPole-v1_2024-01-12_11-48-40_0a791feb/manager_obj.pickle' \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "my_experiment.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3477740-bfe4-4e0b-8a5d-cd0206786c8b",
   "metadata": {},
   "source": [
    "### Record a video of the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da8d90cc-2d77-4682-a97c-a3dacd081f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Evaluation:Warning: n_simulations parameter in eval_kwargs is being overwritten with 1."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving video to ./logs/videos/FQI_CartPole-v1.mp4\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No ffmpeg exe could be found. Install ffmpeg on your system, or set the IMAGEIO_FFMPEG_EXE environment variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m video_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFQI_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m n_eval_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmy_experiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_agents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_simulations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rltp/lib/python3.9/site-packages/rlberry/manager/experiment_manager.py:591\u001b[0m, in \u001b[0;36mExperimentManager.eval_agents\u001b[0;34m(self, n_simulations, eval_kwargs, agent_id, verbose)\u001b[0m\n\u001b[1;32m    587\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    588\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: n_simulations parameter in eval_kwargs is being overwritten with 1.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    589\u001b[0m     )\n\u001b[1;32m    590\u001b[0m eval_kwargs_with_n_simulations[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_simulations\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 591\u001b[0m values\u001b[38;5;241m.\u001b[39mappend(\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meval_kwargs_with_n_simulations\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mgetEffectiveLevel() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m:  \u001b[38;5;66;03m# If debug\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 147\u001b[0m, in \u001b[0;36mFitted_Q_Iteration.eval\u001b[0;34m(self, n_simulations, video_name)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m video_recorder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving video to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_recorder\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 147\u001b[0m     \u001b[43mvideo_recorder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal reward = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(episode_returns)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m +/- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mstd(episode_returns)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m )\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(episode_returns)\n",
      "File \u001b[0;32m~/miniforge3/envs/rltp/lib/python3.9/site-packages/gymnasium/wrappers/monitoring/video_recorder.py:149\u001b[0m, in \u001b[0;36mVideoRecorder.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecorded_frames) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmoviepy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvideo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mImageSequenceClip\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageSequenceClip\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mDependencyNotInstalled(\n\u001b[1;32m    152\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmoviepy is not installed, run `pip install moviepy`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/rltp/lib/python3.9/site-packages/moviepy/video/io/ImageSequenceClip.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimageio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m imread\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mVideoClip\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VideoClip\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mImageSequenceClip\u001b[39;00m(VideoClip):\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    A VideoClip made from a series of images.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/rltp/lib/python3.9/site-packages/moviepy/video/VideoClip.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mClip\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Clip\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DEVNULL, string_types\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_setting\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (add_mask_if_none, apply_to_mask,\n\u001b[1;32m     20\u001b[0m                           convert_masks_to_RGB, convert_to_seconds, outplace,\n\u001b[1;32m     21\u001b[0m                           requires_duration, use_clip_fps_by_default)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (deprecated_version_of, extensions_dict, find_extension,\n\u001b[1;32m     23\u001b[0m                      is_string, subprocess_call)\n",
      "File \u001b[0;32m~/miniforge3/envs/rltp/lib/python3.9/site-packages/moviepy/config.py:36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m FFMPEG_BINARY\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mffmpeg-imageio\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimageio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mffmpeg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_exe\n\u001b[0;32m---> 36\u001b[0m     FFMPEG_BINARY \u001b[38;5;241m=\u001b[39m \u001b[43mget_exe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m FFMPEG_BINARY\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto-detect\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m try_cmd([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mffmpeg\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/miniforge3/envs/rltp/lib/python3.9/site-packages/imageio/plugins/ffmpeg.py:173\u001b[0m, in \u001b[0;36mget_exe\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_exe\u001b[39m():  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for imageio_ffmpeg.get_ffmpeg_exe()\"\"\"\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimageio_ffmpeg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_ffmpeg_exe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rltp/lib/python3.9/site-packages/imageio_ffmpeg/_utils.py:34\u001b[0m, in \u001b[0;36mget_ffmpeg_exe\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exe\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Nothing was found\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo ffmpeg exe could be found. Install ffmpeg on your system, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor set the IMAGEIO_FFMPEG_EXE environment variable.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No ffmpeg exe could be found. Install ffmpeg on your system, or set the IMAGEIO_FFMPEG_EXE environment variable."
     ]
    }
   ],
   "source": [
    "video_name = f\"FQI_{env_id}\"\n",
    "n_eval_episodes = 3\n",
    "my_experiment.eval_agents(\n",
    "    eval_kwargs=dict(n_simulations=n_eval_episodes, video_name=video_name)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84c6d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "\n",
    "def show_videos(video_path: str = \"\", prefix: str = \"\") -> None:\n",
    "    \"\"\"\n",
    "    Taken from https://github.com/eleurent/highway-env\n",
    "\n",
    "    :param video_path: Path to the folder containing videos\n",
    "    :param prefix: Filter the video, showing only the only starting with this prefix\n",
    "    \"\"\"\n",
    "    html = []\n",
    "    for mp4 in Path(video_path).glob(f\"{prefix}*.mp4\"):\n",
    "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
    "        html.append(\n",
    "            \"\"\"<video alt=\"{}\" autoplay\n",
    "                    loop controls style=\"height: 400px;\">\n",
    "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
    "                </video>\"\"\".format(\n",
    "                mp4, video_b64.decode(\"ascii\")\n",
    "            )\n",
    "        )\n",
    "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc66910-de41-4622-9811-96addf8ba8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"FQI agent on {env_id} after {n_iterations} iterations:\")\n",
    "show_videos(\"./logs/videos/\", prefix=video_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a60230-6477-4318-8d60-10f6eada6064",
   "metadata": {},
   "source": [
    "### Going further (optional)\n",
    "\n",
    "- play with different models, and with their hyperparameters\n",
    "- play with the discount factor\n",
    "- play with the number of data collected/used\n",
    "- combine data from random policy with data from trained model\n",
    "- Use a neural network as the regression model (Scikit-learn has a class for simple fully connected neural networks)\n",
    "- Implement DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b8a44f-d48c-4a9d-8ed4-968c1ffb9948",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "What we have seen in this notebook:\n",
    "- collecting data using a random agent in a gym environment\n",
    "- predicting q-values using a regression model\n",
    "- the fitted q-iteration (FQI) algorithm to learn from an offline dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
