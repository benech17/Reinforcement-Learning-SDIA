{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "- Follow the installation instructions in the readme file\n",
    "- Answer the questions in this notebook\n",
    "- Once your work is finished: restart the kernel, run all cells in order and check that the outputs are correct.\n",
    "- Send your completed notebook to `remy.degenne@inria.fr` with email title `SL_TP1_NAME1_NAME2` (or `SL_TP1_NAME` if you work alone).\n",
    "\n",
    "**Deadline: Friday, December 8, 11:59 CET**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't want to use a local installation, you can try Google Colab:\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/RemyDegenne/remydegenne.github.io/blob/master/docs/SL_2023/StoreManagement.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is setting up google colab. Ignore it if you work locally.\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  print(\"Installing packages, please wait a few moments. Restart the runtime after the installation.\")\n",
    "  # install rlberry library\n",
    "  !pip install scipy rlberry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Management MDP\n",
    "\n",
    "You own a bike store. During week $t$, the (random) demand is $D_t$ units. \n",
    "On Monday morning you may choose to command $A_t$ additional units: they are delivered immediately before the shop opens. For each week:\n",
    "\n",
    "  * Maintenance cost: $h$ per unit in stock left from the previous week (no maintenance is needed for newly commanded items)\n",
    "  * Command cost: $c$ for each unit ordered + $c_0$ per command\n",
    "  * Sales profit: $p$ per unit sold\n",
    "  * Constraint: \n",
    "    - your warehouse has a maximal capacity of $M$ unit (any additionnal bike gets stolen)\n",
    "    - you cannot sell bikes that you don't have in stock\n",
    "\n",
    "\n",
    "* State: number of bikes in stock left from the last week => state space $\\mathcal{S} = \\{0,\\dots, M\\}$\n",
    "* Action: number of bikes commanded at the beginning of the week => action space $\\mathcal{A} = \\{0, \\dots ,M\\}$ \n",
    "* Reward = balance of the week: if you command $a_t$ bikes,\n",
    "$$r_t = -c_0 \\mathbb{1}_{(a_t >0)}- c \\times a_t - h\\times s_t + p \\times \\min(D_t, s_t+a_t, M)$$\n",
    "* Transitions: you end the week with the number of bikes $$s_{t+1} = \\max\\big(0, \\min(M, s_t + a_t)  - D_t \\big)$$ \n",
    "\n",
    "Our goal is to maximize the discounted sum of rewards, starting from an initial stock $s_1$, that is to find a policy whose value is \n",
    "$$V^*(s_1) = \\max_{\\pi}\\mathbb{E}_{\\pi}\\left[\\sum_{s=1}^{\\infty} \\gamma^{s-1}r_s \\right].$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "import gymnasium as gym\n",
    "from matplotlib import pyplot as plt\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 15 # stock capacity\n",
    "h = 0.3 # maintenance cost (per unit)\n",
    "c = 0.5 # ordering cost (per unit)\n",
    "c0 = 0.3 # fix delivery cost per command\n",
    "p = 1 # selling price (per unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying the demand distribution \n",
    "\n",
    "As an example of demand distribution, we choose a (truncated) geometric distribution, for which \n",
    "$$\\mathbb{P}(D_t = m) = q(1-q)^m \\ \\ \\forall m \\in \\{0,\\dots,M-1\\}$$\n",
    "and $\\mathbb{P}(D_t = M) = 1 - \\sum_{m=0}^{M-1}\\mathbb{P}(D_t = m)$. We provide below a function that simulate the demand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demand distribution (truncated geometric with parameter q)\n",
    "q = 0.1\n",
    "pdem = np.array([q*(1-q)**m for m in range(M+1)])\n",
    "pdem[M] = pdem[M]+1-np.sum(pdem)\n",
    "\n",
    "print(\"the average demand is \",np.sum([m*pdem[m] for m in range(M+1)]))\n",
    "\n",
    "def SimuDemand(pdem): \n",
    "    cpdem = np.cumsum(pdem)\n",
    "    i=0\n",
    "    u = rd.random()\n",
    "    while (u >cpdem[i]):\n",
    "        i = i+1\n",
    "    return i \n",
    "\n",
    "print(\"a simulated demand is \",SimuDemand(pdem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the MDP as a gymnasium environment\n",
    "\n",
    "This is just a  toy example on how to create a gymnasium environement. \n",
    "\n",
    "Note that our environement is an example of tabular MDP for which the transition probabilities $P(s'|s,a)$ and the mean rewards $r(s,a)$ can actually be computed in closed form. Therefore we provide the transitions $(P)$ and mean rewards $(r)$ as attributes of the class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nextstate(s,a,d,M):\n",
    "    # computes the next state if the demand is d\n",
    "    return max(0,min(M,s+a) -d)\n",
    "\n",
    "def nextreward(s,a,d,M,c,c0,h,p):\n",
    "    # computes the reward if the demand is d\n",
    "    rew = -c*a - h*s + p*min(M,d,s+a)\n",
    "    if (a>0):\n",
    "        rew = rew - c0\n",
    "    return rew\n",
    "\n",
    "class StoreManagement(gym.Env):\n",
    "    \"\"\"\n",
    "    Retail Store Management environment\n",
    "    The environment defines which actions can be taken at which point and\n",
    "    when the agent receives which reward.\n",
    "    \"\"\"\n",
    "    def __init__(self,FirstState,M=15,h=0.3,c=0.5,c0=0.3,p=1,q=0.1):\n",
    "        \n",
    "        # General variables defining the environment\n",
    "        self.Stock_Capacity = M\n",
    "        self.Maintenance_Cost = h\n",
    "        self.Order_Cost = c \n",
    "        self.Delivery_Cost = c0\n",
    "        self.Selling_Price = p\n",
    "        pdem = np.array([q*(1-q)**m for m in range(M+1)])\n",
    "        pdem[M] = pdem[M]+1-np.sum(pdem)\n",
    "        self.Demand_Distribution = pdem\n",
    "        \n",
    "        # Define the action space\n",
    "        self.action_space = gym.spaces.Discrete(self.Stock_Capacity+1)\n",
    "\n",
    "        # Define the state space (state space = observation space in this example)\n",
    "        self.observation_space = gym.spaces.Discrete(self.Stock_Capacity+1)\n",
    "\n",
    "        # current time step\n",
    "        self.curr_step = -1 # \n",
    "        \n",
    "        # initial state\n",
    "        self.state = FirstState\n",
    "\n",
    "        # computation of the MDP parameters\n",
    "        P = np.zeros((M+1,M+1,M+1)) # P[s,a,s'] = p(s' | s,a) \n",
    "        r = np.zeros((M+1,M+1)) # r[s,a] = average reward received in state s when playing action a\n",
    "        ## iteration over all possible states, actions, and possible demand values\n",
    "        for a in range(M+1):\n",
    "            for s in range(M+1):\n",
    "                for d in range(M+1):\n",
    "                    # next state and reward with demand d\n",
    "                    ns = max(0,min(M,s+a) -d)\n",
    "                    reward = -c*a - h*s+p*min(M,d,s+a)\n",
    "                    if (a>0):\n",
    "                        reward = reward - c0\n",
    "                    P[s,a,ns] += pdem[d]\n",
    "                    r[s,a] += pdem[d] * reward\n",
    "        self.P = P # P[s,a,ns] = P(ns | s,a)\n",
    "        self.r = r # r[s,a] = r(s,a)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        simulates a transition following an action in the current state\n",
    "        action : int\n",
    "        \"\"\"\n",
    "        self.curr_step += 1\n",
    "        # simulate the demand \n",
    "        Demand = SimuDemand(self.Demand_Distribution)\n",
    "        # compute the reward\n",
    "        reward = nextreward(self.state,action,Demand,self.Stock_Capacity,self.Order_Cost,self.Delivery_Cost,self.Maintenance_Cost,self.Selling_Price)\n",
    "        # compute the next state \n",
    "        self.state = nextstate(self.state,action,Demand,self.Stock_Capacity)\n",
    "        # return 4 elements: observation / reward / termination? / truncation ? / information  \n",
    "        return self.state, reward, False, False,{}\n",
    "\n",
    "    def reset(self,InitialStock):\n",
    "        \"\"\"\n",
    "        Reset the state of the environment and returns an initial observation.\n",
    "        \"\"\"\n",
    "        self.curr_step = -1\n",
    "        self.state = InitialStock\n",
    "    \n",
    "    def _render(self, mode='human'):\n",
    "        \"\"\"optional visualization of the interaction: none here\"\"\"\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function that simulates a trajectory under a policy Pi starting from some state $s_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete the function below to simulate a trajectory of length $T$ from an initial state $s_0$ under a policy Pi**\n",
    "\n",
    "Write a function `SimulateTrajectory` that takes 3 arguments T, Pi and s0. It should initialize a StoreManagement environment in the state s0. Then, for T steps, will will:\n",
    "- query the state of the environment\n",
    "- choose an action according to the policy\n",
    "- get a new state and a reward by calling `state, reward, _, _, _ = env.step(action)` (where `env` is the environment)\n",
    "\n",
    "The function should return two arrays of length `T`: one for the states visited and one for the rewards obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimulateTrajectory(T,Pi,s0):\n",
    "    \"\"\"return a vector of T successive states and a vector of T successive rewards\"\"\"\n",
    "    Rewards = np.zeros(T)\n",
    "    States = np.zeros(T)\n",
    "    env=StoreManagement(s0)\n",
    "    for t in range(T):\n",
    "        # ** TO BE COMPLETED **\n",
    "    return States, Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running simulations with three simple baselines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = 10 # initial stock \n",
    "gamma = 0.97 # discount factor \n",
    "\n",
    "def PiUniform(s):\n",
    "    # pick uniformly at random in {0,1,...,M-s}\n",
    "    x = rd.sample(range(M+1-s),1)\n",
    "    return s+x[0]\n",
    "\n",
    "def PiConstant(s,c=3):\n",
    "    # oder a constant number of c bikes \n",
    "    return min(c,M-s)\n",
    "\n",
    "def PiThreshold(s,m1=4,m2=10):\n",
    "    # if less than m1 bikes in stock, refill it up to m2\n",
    "    action = 0\n",
    "    if (s <=m1):\n",
    "        action = (m2-s)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the cumulative discounted reward as a function of time for the three baselines.**\n",
    "\n",
    "You should plot one trajectory for each policy, all on the same graph. Don't forget to add legends to the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = int(np.log(1/(0.001*(1-gamma)))/np.log(1/gamma)) # truncation of the infinite sum\n",
    "\n",
    "# ** TO BE COMPLETED **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolution of the stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(States3[range(100)])\n",
    "plt.xlabel('weeks')\n",
    "plt.ylabel('stock')\n",
    "plt.title('Evolution of the stock under a threshold policy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming\n",
    "\n",
    "So far, when doing simulations, we just interacted with the environements with calls to the \"step\" functions (which boils down to simulating the demand each week). However, we assume that all the parameters (including the demand distribution itself, paramterized by $q$) are known, we can evaluate policies, and compute the optimal policy using the knowledge of the MDP parameters (encoded in env.P and env.r)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StoreManagement(0,M=15,h=0.3,c=0.5,c0=0.3,p=1,q=0.1)\n",
    "\n",
    "# MDP parameters\n",
    "P = env.P # P[s,a,s'] = p(s' | s,a) \n",
    "r = env.r # r[s,a] = average reward received in state s when playing action a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of deterministic policies\n",
    "\n",
    "**Evalute a policy of your choice using Monte-Carlo simulation, i.e. compute the value of this policy in all states.** \n",
    "\n",
    "In a Monte-Carlo simulation to estimate the value of $V^\\pi(s)$, we collect `MC` values for the sum of discounted rewards over long enough trajectories starting at $s$, then compute the mean of these `MC` values.\n",
    "We need to compute trajectories from *all* states.\n",
    "\n",
    "Observe that we do *not* need the knowledge of $P$ and $r$ for this method, simulating trajectories is enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Computes the values in each state by Monte-Carlo simulation (needs many simulations to be precise)\n",
    "\n",
    "Policy = PiThreshold\n",
    "\n",
    "MC = 100 # number of Monte-Carlo simulations\n",
    "Values = np.zeros(M+1)\n",
    "\n",
    "# ** TO BE COMPLETED **\n",
    "\n",
    "print(\"the estimated value in each states for the threshold policy are:\")\n",
    "\n",
    "print(Values)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement a function that evaluates a deterministic policy Pi using the matrix inversion technique. Compare the output with the previous method for the same policy.**\n",
    "\n",
    "Unlike the previous evaluation method, this one requires the knowledge of $P$ and $r$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluatePolicy(Pi):\n",
    "    # input: function (could also be vector as we can only use this method for deterministic policies)\n",
    "    # ** TO BE COMPLETED **\n",
    "    # should return a vector of values\n",
    "\n",
    "Policy = PiThreshold\n",
    "Values = EvaluatePolicy(Policy)\n",
    "\n",
    "print(Values)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate two different policies and visualize their values for the different states in order to decide which one is better.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** TO BE COMPLETED **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Optimal Policy: Policy Iteration \n",
    "\n",
    "The function given below performs policy improvement, and may be useful to implement both value iteration and policy iteration. Note that the (deterministic) policy it outputs is encoded as a vector, not a function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy improvement\n",
    "def Improve(V):\n",
    "    '''computes the Q table associated to V and \n",
    "    return Pi=greedy(V)=greedy(Q) as well as max_{a} Q[s,a]'''\n",
    "    Pi = np.zeros(M+1) # improved policy \n",
    "    newV = np.zeros(M+1)\n",
    "    # compute the Q table \n",
    "    Q = np.zeros((M+1,M+1))\n",
    "    for s in range(M+1):\n",
    "        for a in range(M+1):\n",
    "            Q[s,a]=r[s,a]+gamma*np.sum([P[s,a,ns]*V[ns] for ns in range(M+1)])\n",
    "        # improvement (greedy policy wrt to Q)\n",
    "        pi = np.argmax(Q[s,:])\n",
    "        Pi[s]=pi\n",
    "        newV[s]=Q[s,pi]\n",
    "        Pi=Pi.astype(int)\n",
    "    return Pi,newV\n",
    "\n",
    "\n",
    "V = np.random.randint(M+1,size=M+1) \n",
    "newPi,newV = Improve(V)\n",
    "print(\"improved policy from random:\",newPi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement Value Iteration and Policy Iteration.**\n",
    "\n",
    "First, check that the two algorithms output the same optimal policy. Then, you may compare the number of iteration needed by both algorithms as well as the executation time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ValueIteration(epsilon=1e-3): # epsilon = guaranteed precision\n",
    "    # ** TO BE COMPLETED **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolicyIteration():\n",
    "    # ** TO BE COMPLETED **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = time.time()\n",
    "Pi,V,nIt = ValueIteration()\n",
    "elapsed = time.time()-start\n",
    "\n",
    "start = time.time()\n",
    "Pi2,V2,nIt2 = PolicyIteration()\n",
    "elapsed = time.time()-start\n",
    "\n",
    "print(\"Value iteration yields policy\",Pi,\"and value \",V,\" in \",nIt,\" iterations and t=\",elapsed,\" seconds\\n\")\n",
    "print(\"Optimal policy is\",Pi2,\"with value \",V2,\" in \",nIt2,\" iterations and t=\",elapsed,\" seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a visualization of the optimal policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Pi2)\n",
    "plt.xlabel('stock')\n",
    "plt.ylabel('order under the optimal policy')\n",
    "plt.title(\"Optimal Policy\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(V2)\n",
    "plt.xlabel('stock')\n",
    "plt.ylabel('value of the optimal policy')\n",
    "plt.title(\"Optimal Value\")\n",
    "\n",
    "Vstar=np.copy(V)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
